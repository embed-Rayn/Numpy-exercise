{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선형 회귀(Linear Regression)\n",
    "1. 선형 회귀\n",
    "2. 자동 미분\n",
    "3. 다중 선형 회귀\n",
    "4. nn.Module로 구현하는 선형 회귀\n",
    "5. 클래스로 파이토치 모델 구현하기\n",
    "6. 미니 배치와 데이터 로드\n",
    "7. 커스텀 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 선형 회귀의 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2410fda1270>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 변수선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 가중치와 편향의 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "W = torch.zeros(1, requires_grad=True) \n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# 가중치 W를 출력\n",
    "print(W) \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 가설 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = W * x_train + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 비용 함수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2) \n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. 경사 하강법 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 항상 따라다니는 코드(epoch마다)\\n# gradient를 0으로 초기화\\noptimizer.zero_grad() \\n# 비용 함수를 미분하여 gradient 계산\\ncost.backward() \\n# W와 b를 업데이트\\noptimizer.step() \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 항상 따라다니는 코드(epoch마다)\n",
    "# gradient를 0으로 초기화\n",
    "optimizer.zero_grad() \n",
    "# 비용 함수를 미분하여 gradient 계산\n",
    "cost.backward() \n",
    "# W와 b를 업데이트\n",
    "optimizer.step() \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n",
      "Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n",
      "Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n",
      "Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n",
      "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n",
      "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n",
      "Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n",
      "Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n",
      "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n",
      "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n",
      "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n",
      "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n",
      "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n",
      "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n",
      "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n",
      "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
      "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
      "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
      "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n",
      "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
      "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "# 모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x_train * W + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.자동 미분 실습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 자동 미분(Autograd) 실습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값 : 8.0\n"
     ]
    }
   ],
   "source": [
    "# 텐서에 대한 기울기를 저장하겠다는 의미\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = w**2\n",
    "z = 2*y + 5\n",
    "\n",
    "# 해당 수식을 w에 대하여 미분\n",
    "z.backward()\n",
    "\n",
    "print('수식을 w로 미분한 값 : {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.다중 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2410fda1270>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터\n",
    "x1_train = torch.FloatTensor([[77], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[88], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 w1: 0.296 w2: 0.297 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
      "Epoch  100/1000 w1: 0.676 w2: 0.682 w3: 0.682 b: 0.008 Cost: 13.648926\n",
      "Epoch  200/1000 w1: 0.690 w2: 0.698 w3: 0.698 b: 0.007 Cost: 12.305525\n",
      "Epoch  300/1000 w1: 0.702 w2: 0.714 w3: 0.714 b: 0.007 Cost: 11.097908\n",
      "Epoch  400/1000 w1: 0.714 w2: 0.729 w3: 0.729 b: 0.007 Cost: 10.012353\n",
      "Epoch  500/1000 w1: 0.725 w2: 0.744 w3: 0.744 b: 0.007 Cost: 9.036519\n",
      "Epoch  600/1000 w1: 0.736 w2: 0.757 w3: 0.757 b: 0.007 Cost: 8.159314\n",
      "Epoch  700/1000 w1: 0.746 w2: 0.770 w3: 0.770 b: 0.007 Cost: 7.370742\n",
      "Epoch  800/1000 w1: 0.755 w2: 0.782 w3: 0.782 b: 0.007 Cost: 6.661847\n",
      "Epoch  900/1000 w1: 0.764 w2: 0.794 w3: 0.794 b: 0.007 Cost: 6.024539\n",
      "Epoch 1000/1000 w1: 0.773 w2: 0.805 w3: 0.805 b: 0.007 Cost: 5.451605\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, w1.item(), w3.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬 연산을 고려하여 파이토치로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
      "Epoch    1/20 hypothesis: tensor([67.2550, 80.8370, 79.6496, 86.7367, 61.6576]) Cost: 9299.041992\n",
      "Epoch    2/20 hypothesis: tensor([104.9094, 126.0961, 124.2437, 135.2988,  96.1786]) Cost: 2916.040283\n",
      "Epoch    3/20 hypothesis: tensor([125.9912, 151.4358, 149.2109, 162.4876, 115.5064]) Cost: 915.195618\n",
      "Epoch    4/20 hypothesis: tensor([137.7942, 165.6231, 163.1894, 177.7099, 126.3278]) Cost: 288.001404\n",
      "Epoch    5/20 hypothesis: tensor([144.4023, 173.5665, 171.0157, 186.2326, 132.3867]) Cost: 91.397415\n",
      "Epoch    6/20 hypothesis: tensor([148.1018, 178.0139, 175.3974, 191.0042, 135.7791]) Cost: 29.768749\n",
      "Epoch    7/20 hypothesis: tensor([150.1729, 180.5040, 177.8505, 193.6756, 137.6786]) Cost: 10.449845\n",
      "Epoch    8/20 hypothesis: tensor([151.3323, 181.8983, 179.2240, 195.1713, 138.7423]) Cost: 4.393543\n",
      "Epoch    9/20 hypothesis: tensor([151.9812, 182.6791, 179.9928, 196.0086, 139.3380]) Cost: 2.494627\n",
      "Epoch   10/20 hypothesis: tensor([152.3443, 183.1164, 180.4233, 196.4774, 139.6717]) Cost: 1.898885\n",
      "Epoch   11/20 hypothesis: tensor([152.5474, 183.3614, 180.6642, 196.7398, 139.8587]) Cost: 1.711626\n",
      "Epoch   12/20 hypothesis: tensor([152.6610, 183.4986, 180.7990, 196.8866, 139.9636]) Cost: 1.652449\n",
      "Epoch   13/20 hypothesis: tensor([152.7243, 183.5757, 180.8745, 196.9688, 140.0226]) Cost: 1.633397\n",
      "Epoch   14/20 hypothesis: tensor([152.7596, 183.6189, 180.9166, 197.0147, 140.0557]) Cost: 1.626925\n",
      "Epoch   15/20 hypothesis: tensor([152.7792, 183.6432, 180.9402, 197.0404, 140.0745]) Cost: 1.624401\n",
      "Epoch   16/20 hypothesis: tensor([152.7899, 183.6570, 180.9533, 197.0547, 140.0851]) Cost: 1.623121\n",
      "Epoch   17/20 hypothesis: tensor([152.7958, 183.6648, 180.9606, 197.0627, 140.0913]) Cost: 1.622214\n",
      "Epoch   18/20 hypothesis: tensor([152.7988, 183.6694, 180.9646, 197.0671, 140.0949]) Cost: 1.621441\n",
      "Epoch   19/20 hypothesis: tensor([152.8004, 183.6720, 180.9668, 197.0695, 140.0972]) Cost: 1.620692\n",
      "Epoch   20/20 hypothesis: tensor([152.8010, 183.6736, 180.9680, 197.0708, 140.0986]) Cost: 1.619987\n"
     ]
    }
   ],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "# 모델 초기화\n",
    "W = torch.zeros((3, 1), requires_grad=True)\n",
    "b = torch.zeros((5, 1), requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    # 편향 b는 브로드 캐스팅되어 각 샘플에 더해집니다.\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "\n",
    "    # cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.nn.Module로 구현하는 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 단순 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2410fda1270>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1,1)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/2000 Cost: 1.237737\n",
      "Epoch  200/2000 Cost: 6.770324\n",
      "Epoch  300/2000 Cost: 12.201328\n",
      "Epoch  400/2000 Cost: 0.356613\n",
      "Epoch  500/2000 Cost: 8.695301\n",
      "Epoch  600/2000 Cost: 10.845249\n",
      "Epoch  700/2000 Cost: 0.008479\n",
      "Epoch  800/2000 Cost: 10.454887\n",
      "Epoch  900/2000 Cost: 9.149695\n",
      "Epoch 1000/2000 Cost: 0.217620\n",
      "Epoch 1100/2000 Cost: 11.909933\n",
      "Epoch 1200/2000 Cost: 7.246561\n",
      "Epoch 1300/2000 Cost: 0.976930\n",
      "Epoch 1400/2000 Cost: 12.929339\n",
      "Epoch 1500/2000 Cost: 5.302049\n",
      "Epoch 1600/2000 Cost: 2.219204\n",
      "Epoch 1700/2000 Cost: 13.427772\n",
      "Epoch 1800/2000 Cost: 3.482737\n",
      "Epoch 1900/2000 Cost: 3.833276\n",
      "Epoch 2000/2000 Cost: 13.373055\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:6f}'.format(\n",
    "        epoch, nb_epochs, cost.item()))\n",
    "        #print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[3.4427]], requires_grad=True), Parameter containing:\n",
      "tensor([0.4733], requires_grad=True)]\n",
      "tensor([[69.3268]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))\n",
    "print(model(torch.FloatTensor([[20]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 다중 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2410fda1270>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  100/2000 Cost: 0.226011\n",
      "Epoch  200/2000 Cost: 0.223930\n",
      "Epoch  300/2000 Cost: 0.221953\n",
      "Epoch  400/2000 Cost: 0.220074\n",
      "Epoch  500/2000 Cost: 0.218288\n",
      "Epoch  600/2000 Cost: 0.216593\n",
      "Epoch  700/2000 Cost: 0.214968\n",
      "Epoch  800/2000 Cost: 0.213431\n",
      "Epoch  900/2000 Cost: 0.211972\n",
      "Epoch 1000/2000 Cost: 0.210572\n",
      "Epoch 1100/2000 Cost: 0.209247\n",
      "Epoch 1200/2000 Cost: 0.207987\n",
      "Epoch 1300/2000 Cost: 0.206780\n",
      "Epoch 1400/2000 Cost: 0.205629\n",
      "Epoch 1500/2000 Cost: 0.204536\n",
      "Epoch 1600/2000 Cost: 0.203490\n",
      "Epoch 1700/2000 Cost: 0.202495\n",
      "Epoch 1800/2000 Cost: 0.201550\n",
      "Epoch 1900/2000 Cost: 0.200642\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Linear(3, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
    "nb_epochs = 2000\n",
    "\n",
    "for epoch in range(1, nb_epochs):\n",
    "    cost = torch.nn.functional.mse_loss(y_train, model(x_train))\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.9778, 0.4538, 0.5768]], requires_grad=True), Parameter containing:\n",
      "tensor([0.2802], requires_grad=True)]\n",
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2304]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))\n",
    "\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05. 클래스로 파이토치 모델 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 모델을 클래스로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module): # torch.nn.Module을 상속받는 파이썬 클래스\n",
    "    def __init__(self): #\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1) # 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 단순 선형 회귀 클래스로 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 13.103540\n",
      "Epoch  100/2000 Cost: 0.002791\n",
      "Epoch  200/2000 Cost: 0.001724\n",
      "Epoch  300/2000 Cost: 0.001066\n",
      "Epoch  400/2000 Cost: 0.000658\n",
      "Epoch  500/2000 Cost: 0.000407\n",
      "Epoch  600/2000 Cost: 0.000251\n",
      "Epoch  700/2000 Cost: 0.000155\n",
      "Epoch  800/2000 Cost: 0.000096\n",
      "Epoch  900/2000 Cost: 0.000059\n",
      "Epoch 1000/2000 Cost: 0.000037\n",
      "Epoch 1100/2000 Cost: 0.000023\n",
      "Epoch 1200/2000 Cost: 0.000014\n",
      "Epoch 1300/2000 Cost: 0.000009\n",
      "Epoch 1400/2000 Cost: 0.000005\n",
      "Epoch 1500/2000 Cost: 0.000003\n",
      "Epoch 1600/2000 Cost: 0.000002\n",
      "Epoch 1700/2000 Cost: 0.000001\n",
      "Epoch 1800/2000 Cost: 0.000001\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "model = LinearRegressionModel()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "    h = model.forward(x_train)\n",
    "    cost = nn.functional.mse_loss(h, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2410fda1270>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiVariableLinearRegression(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 / 2000: \tcost: 31667.597656\n",
      " 100 / 2000: \tcost: 0.225993\n",
      " 200 / 2000: \tcost: 0.223911\n",
      " 300 / 2000: \tcost: 0.221941\n",
      " 400 / 2000: \tcost: 0.220059\n",
      " 500 / 2000: \tcost: 0.218271\n",
      " 600 / 2000: \tcost: 0.216575\n",
      " 700 / 2000: \tcost: 0.214950\n",
      " 800 / 2000: \tcost: 0.213413\n",
      " 900 / 2000: \tcost: 0.211952\n",
      "1000 / 2000: \tcost: 0.210559\n",
      "1100 / 2000: \tcost: 0.209230\n",
      "1200 / 2000: \tcost: 0.207967\n",
      "1300 / 2000: \tcost: 0.206762\n",
      "1400 / 2000: \tcost: 0.205618\n",
      "1500 / 2000: \tcost: 0.204529\n",
      "1600 / 2000: \tcost: 0.203481\n",
      "1700 / 2000: \tcost: 0.202486\n",
      "1800 / 2000: \tcost: 0.201539\n",
      "1900 / 2000: \tcost: 0.200634\n",
      "2000 / 2000: \tcost: 0.199770\n"
     ]
    }
   ],
   "source": [
    "model = MultiVariableLinearRegression(3, 1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5 )\n",
    "nb_epochs = 2000\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    predict = model.forward(x_train)\n",
    "    cost = torch.mean(nn.functional.mse_loss(predict, y_train))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"{:4d} / {}: \\tcost: {:.6f}\".format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.미니배치와 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
    "from torch.utils.data import DataLoader # 데이터로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20 Batch 1/3 Cost: 46978.785156\n",
      "Epoch    0/20 Batch 2/3 Cost: 6497.784668\n",
      "Epoch    0/20 Batch 3/3 Cost: 5174.395996\n",
      "Epoch    1/20 Batch 1/3 Cost: 711.106018\n",
      "Epoch    1/20 Batch 2/3 Cost: 401.381378\n",
      "Epoch    1/20 Batch 3/3 Cost: 212.294891\n",
      "Epoch    2/20 Batch 1/3 Cost: 26.882061\n",
      "Epoch    2/20 Batch 2/3 Cost: 0.057693\n",
      "Epoch    2/20 Batch 3/3 Cost: 41.471272\n",
      "Epoch    3/20 Batch 1/3 Cost: 13.444824\n",
      "Epoch    3/20 Batch 2/3 Cost: 14.656877\n",
      "Epoch    3/20 Batch 3/3 Cost: 5.495739\n",
      "Epoch    4/20 Batch 1/3 Cost: 6.619390\n",
      "Epoch    4/20 Batch 2/3 Cost: 19.459251\n",
      "Epoch    4/20 Batch 3/3 Cost: 16.812302\n",
      "Epoch    5/20 Batch 1/3 Cost: 19.544233\n",
      "Epoch    5/20 Batch 2/3 Cost: 21.681820\n",
      "Epoch    5/20 Batch 3/3 Cost: 13.138634\n",
      "Epoch    6/20 Batch 1/3 Cost: 2.338879\n",
      "Epoch    6/20 Batch 2/3 Cost: 21.505651\n",
      "Epoch    6/20 Batch 3/3 Cost: 17.908745\n",
      "Epoch    7/20 Batch 1/3 Cost: 8.261931\n",
      "Epoch    7/20 Batch 2/3 Cost: 12.769864\n",
      "Epoch    7/20 Batch 3/3 Cost: 24.187618\n",
      "Epoch    8/20 Batch 1/3 Cost: 10.529057\n",
      "Epoch    8/20 Batch 2/3 Cost: 14.057606\n",
      "Epoch    8/20 Batch 3/3 Cost: 18.635973\n",
      "Epoch    9/20 Batch 1/3 Cost: 0.210614\n",
      "Epoch    9/20 Batch 2/3 Cost: 22.468868\n",
      "Epoch    9/20 Batch 3/3 Cost: 20.725445\n",
      "Epoch   10/20 Batch 1/3 Cost: 20.487284\n",
      "Epoch   10/20 Batch 2/3 Cost: 7.705010\n",
      "Epoch   10/20 Batch 3/3 Cost: 7.212080\n",
      "Epoch   11/20 Batch 1/3 Cost: 20.363573\n",
      "Epoch   11/20 Batch 2/3 Cost: 8.519358\n",
      "Epoch   11/20 Batch 3/3 Cost: 4.553837\n",
      "Epoch   12/20 Batch 1/3 Cost: 6.104103\n",
      "Epoch   12/20 Batch 2/3 Cost: 13.997252\n",
      "Epoch   12/20 Batch 3/3 Cost: 27.482769\n",
      "Epoch   13/20 Batch 1/3 Cost: 7.775980\n",
      "Epoch   13/20 Batch 2/3 Cost: 13.109329\n",
      "Epoch   13/20 Batch 3/3 Cost: 23.921347\n",
      "Epoch   14/20 Batch 1/3 Cost: 5.524771\n",
      "Epoch   14/20 Batch 2/3 Cost: 19.877090\n",
      "Epoch   14/20 Batch 3/3 Cost: 17.504499\n",
      "Epoch   15/20 Batch 1/3 Cost: 11.427785\n",
      "Epoch   15/20 Batch 2/3 Cost: 14.224229\n",
      "Epoch   15/20 Batch 3/3 Cost: 24.827265\n",
      "Epoch   16/20 Batch 1/3 Cost: 22.178373\n",
      "Epoch   16/20 Batch 2/3 Cost: 14.530046\n",
      "Epoch   16/20 Batch 3/3 Cost: 10.774013\n",
      "Epoch   17/20 Batch 1/3 Cost: 21.197256\n",
      "Epoch   17/20 Batch 2/3 Cost: 11.078669\n",
      "Epoch   17/20 Batch 3/3 Cost: 14.773710\n",
      "Epoch   18/20 Batch 1/3 Cost: 8.447533\n",
      "Epoch   18/20 Batch 2/3 Cost: 20.012682\n",
      "Epoch   18/20 Batch 3/3 Cost: 7.388034\n",
      "Epoch   19/20 Batch 1/3 Cost: 14.731810\n",
      "Epoch   19/20 Batch 2/3 Cost: 7.791116\n",
      "Epoch   19/20 Batch 3/3 Cost: 18.635447\n",
      "Epoch   20/20 Batch 1/3 Cost: 17.549286\n",
      "Epoch   20/20 Batch 2/3 Cost: 7.824885\n",
      "Epoch   20/20 Batch 3/3 Cost: 24.961105\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        # print(batch_idx)\n",
    "        # print(samples)\n",
    "        x_train, y_train = samples\n",
    "        # H(x) 계산\n",
    "        prediction = model(x_train)\n",
    "\n",
    "        # cost 계산\n",
    "        cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "        # cost로 H(x) 계산\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
    "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
    "        cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[156.7833]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 [73, 80, 75]를 선언\n",
    "new_var =  torch.FloatTensor([[73, 80, 75]]) \n",
    "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) \n",
    "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.커스텀 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self):\n",
    "    #데이터셋의 전처리를 해주는 부분\n",
    "        self.x_data = [[73, 80, 75],\n",
    "                       [93, 88, 93],\n",
    "                       [89, 91, 90],\n",
    "                       [96, 98, 100],\n",
    "                       [73, 66, 70]]\n",
    "        self.y_data = [[152], [185], [180], [196], [142]]\n",
    "\n",
    "    def __len__(self):\n",
    "    #데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        x = torch.FloatTensor(self.x_data[idx])\n",
    "        y = torch.FloatTensor(self.y_data[idx])\n",
    "        return x, y        \n",
    "    #데이터셋에서 특정 1개의 샘플을 가져오는 함수\n",
    "    \n",
    "    #len(dataset)을 했을 때 데이터셋의 크기를 리턴할 len\n",
    "    #dataset[i]을 했을 때 i번째 샘플을 가져오도록 하는 인덱싱을 위한 get_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(3,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0000/2000\tbatch: 001/003\tcost6319.331543\n",
      "epoch: 0000/2000\tbatch: 002/003\tcost1066.903931\n",
      "epoch: 0000/2000\tbatch: 003/003\tcost796.235840\n",
      "epoch: 0100/2000\tbatch: 001/003\tcost1.625426\n",
      "epoch: 0100/2000\tbatch: 002/003\tcost1.603504\n",
      "epoch: 0100/2000\tbatch: 003/003\tcost3.733830\n",
      "epoch: 0200/2000\tbatch: 001/003\tcost2.425547\n",
      "epoch: 0200/2000\tbatch: 002/003\tcost1.391298\n",
      "epoch: 0200/2000\tbatch: 003/003\tcost0.552497\n",
      "epoch: 0300/2000\tbatch: 001/003\tcost2.272011\n",
      "epoch: 0300/2000\tbatch: 002/003\tcost1.876806\n",
      "epoch: 0300/2000\tbatch: 003/003\tcost0.687435\n",
      "epoch: 0400/2000\tbatch: 001/003\tcost1.741325\n",
      "epoch: 0400/2000\tbatch: 002/003\tcost0.630343\n",
      "epoch: 0400/2000\tbatch: 003/003\tcost1.476142\n",
      "epoch: 0500/2000\tbatch: 001/003\tcost1.645198\n",
      "epoch: 0500/2000\tbatch: 002/003\tcost0.941095\n",
      "epoch: 0500/2000\tbatch: 003/003\tcost0.352050\n",
      "epoch: 0600/2000\tbatch: 001/003\tcost0.980761\n",
      "epoch: 0600/2000\tbatch: 002/003\tcost1.313380\n",
      "epoch: 0600/2000\tbatch: 003/003\tcost0.637371\n",
      "epoch: 0700/2000\tbatch: 001/003\tcost1.142746\n",
      "epoch: 0700/2000\tbatch: 002/003\tcost0.866217\n",
      "epoch: 0700/2000\tbatch: 003/003\tcost0.101635\n",
      "epoch: 0800/2000\tbatch: 001/003\tcost0.247840\n",
      "epoch: 0800/2000\tbatch: 002/003\tcost1.024398\n",
      "epoch: 0800/2000\tbatch: 003/003\tcost1.184868\n",
      "epoch: 0900/2000\tbatch: 001/003\tcost0.989728\n",
      "epoch: 0900/2000\tbatch: 002/003\tcost0.771102\n",
      "epoch: 0900/2000\tbatch: 003/003\tcost0.026067\n",
      "epoch: 1000/2000\tbatch: 001/003\tcost1.612022\n",
      "epoch: 1000/2000\tbatch: 002/003\tcost0.457080\n",
      "epoch: 1000/2000\tbatch: 003/003\tcost0.344988\n",
      "epoch: 1100/2000\tbatch: 001/003\tcost0.847873\n",
      "epoch: 1100/2000\tbatch: 002/003\tcost0.443547\n",
      "epoch: 1100/2000\tbatch: 003/003\tcost0.742202\n",
      "epoch: 1200/2000\tbatch: 001/003\tcost0.088960\n",
      "epoch: 1200/2000\tbatch: 002/003\tcost1.168071\n",
      "epoch: 1200/2000\tbatch: 003/003\tcost1.186197\n",
      "epoch: 1300/2000\tbatch: 001/003\tcost0.652507\n",
      "epoch: 1300/2000\tbatch: 002/003\tcost0.219440\n",
      "epoch: 1300/2000\tbatch: 003/003\tcost0.766934\n",
      "epoch: 1400/2000\tbatch: 001/003\tcost0.610127\n",
      "epoch: 1400/2000\tbatch: 002/003\tcost0.235504\n",
      "epoch: 1400/2000\tbatch: 003/003\tcost0.694207\n",
      "epoch: 1500/2000\tbatch: 001/003\tcost0.671938\n",
      "epoch: 1500/2000\tbatch: 002/003\tcost0.299625\n",
      "epoch: 1500/2000\tbatch: 003/003\tcost0.375141\n",
      "epoch: 1600/2000\tbatch: 001/003\tcost0.226679\n",
      "epoch: 1600/2000\tbatch: 002/003\tcost0.790330\n",
      "epoch: 1600/2000\tbatch: 003/003\tcost0.553473\n",
      "epoch: 1700/2000\tbatch: 001/003\tcost0.502518\n",
      "epoch: 1700/2000\tbatch: 002/003\tcost0.413742\n",
      "epoch: 1700/2000\tbatch: 003/003\tcost0.591239\n",
      "epoch: 1800/2000\tbatch: 001/003\tcost0.465185\n",
      "epoch: 1800/2000\tbatch: 002/003\tcost0.253943\n",
      "epoch: 1800/2000\tbatch: 003/003\tcost0.518841\n",
      "epoch: 1900/2000\tbatch: 001/003\tcost0.406154\n",
      "epoch: 1900/2000\tbatch: 002/003\tcost0.367562\n",
      "epoch: 1900/2000\tbatch: 003/003\tcost0.600001\n",
      "epoch: 2000/2000\tbatch: 001/003\tcost0.663773\n",
      "epoch: 2000/2000\tbatch: 002/003\tcost0.311003\n",
      "epoch: 2000/2000\tbatch: 003/003\tcost0.268424\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 2000\n",
    "for epoch in range(nb_epoch+1):\n",
    "    for batch_idx, samples in enumerate(dataloader):\n",
    "        x, y = samples\n",
    "        h = model(x)\n",
    "        cost = torch.nn.functional.mse_loss(h, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 100 ==0:\n",
    "            print(\"epoch: {:04d}/{:4d}\\tbatch: {:03d}/{:03d}\\tcost{:.6f}\".format(\n",
    "                epoch, nb_epoch, batch_idx + 1, len(dataloader), cost.item())\n",
    "            )\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
